---
title: "Freedom Index Gap Data Project"

output:
  pdf_document:
    latex_engine: tinytex
    toc: true
    toc_depth: 2
---

## About the Data Set and Variables

This data set contains the status of freedom in countries, as well as economic status.

| Variable       | Description                               |
|----------------|-------------------------------------------|
| country        | Name of Country                           |
| year           | Year                                      |
| CL             | Civil Liberties                           |
| PR             | Political Rights                          |
| Status         | Free, Not Free, Partially Free - 3 Levels |
| Region_Code    | UN Region Code                            |
| Region_Name    | UN Region Name                            |
| is_ldc         | Developed country or not - 2 Levels       |
| gdp            | gross domestic product                    |
| dol_per_day    | gdp/population/365                        |
| gdp_per_capita | gdp/population                            |
|                |                                           |

## Required Packages

```{r}
if(!require(tidytuesdayR)) install.packages("tidytuesdayR", repos = "http://cran.us.r-project.org")
library(tidytuesdayR)
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
library(tidyverse)
if(!require(dslabs)) install.packages("dslabs", repos = "http://cran.us.r-project.org")
library(dslabs)
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.library(nnet)org")
library(caret)
if(!require(tidymodels)) install.packages("tidymodels", repos = "http://cran.us.r-project.org")
library(tidymodels)
if(!require(DT)) install.packages("DT", repos = "http://cran.us.r-project.org")
library(DT)
if(!require(ggtheme)) install.packages("ggtheme", repos = "http://cran.us.r-project.org")
library(ggthemes)
if(!require(nnet)) install.packages("nnet", repos = "http://cran.us.r-project.org")
library(nnet)
if(!require(psych)) install.packages("psych", repos = "http://cran.us.r-project.org")
library(psych)
if(!require(corrplot)) install.packages("corrplot", repos = "http://cran.us.r-project.org")
library(corrplot)
if(!require(scatterplot3d)) install.packages("scatterplot3d", repos = "http://cran.us.r-project.org")
library(scatterplot3d)
if(!require(mgcv)) install.packages("mgcv", repos = "http://cran.us.r-project.org")
library(mgcv)
if(!require(Metrics)) install.packages("Metrics", repos = "http://cran.us.r-project.org")
library(Metrics)
if(!require(e1071)) install.packages("e1071", repos = "http://cran.us.r-project.org")
library(e1071)
if(!require(mgcv)) install.packages("mgcv", repos = "http://cran.us.r-project.org")
library(mgcv)
if(!require(rpart)) install.packages("rpart", repos = "http://cran.us.r-project.org")
library(rpart)
if(!require(rpart.plot)) install.packages("rpart.plot", repos = "http://cran.us.r-project.org")
library(rpart.plot)

```

## Data Source

There are 2 sets of data; World Freedom Index from **TidyTuesday**, and **Gapminder**.

TidyTuesday [tidytuesday/data/2022/2022-02-22 at master Â· rfordatascience/tidytuesday (github.com)](https://github.com/rfordatascience/tidytuesday/tree/master/data/2022/2022-02-22)

Gapminder [Gapminder](https://www.gapminder.org/)

```{r}
# World Freedom Index from tidytuesday
tuesdata <- tidytuesdayR::tt_load('2022-02-22')
free <- tuesdata$freedom
# gapminder data set
data("gapminder")
```

## Create A Data Set

We are going to merge 2 data sets to create a new data set for this project.

```{r}
# merge free & gapminder by country and year
free_gap <- merge(free, gapminder, by = c("country", "year"))
# remove Region_Name, Region_Code
free_gap <- free_gap %>%
  dplyr::select(-c(Region_Name, Region_Code)) %>% # specify dplyr to run select
  # factorize Status, is_ldc
  mutate(Status = as.factor(Status)) %>%
  # mutate(is_ldc = as.factor(is_ldc)) %>%
  mutate(country = as.factor(country)) %>%
# create dollar per day and gdb per capita
  filter(!is.na(gdp) & !is.na(population) & !is.na(fertility) & !is.na(infant_mortality)) %>% 
  mutate(dol_per_day = gdp/population/365) %>% #dollar per day
  mutate(gdp_per_cap = gdp/population) #gdp per capita
View(free_gap)
```

## Data Exploration (Variable & Predictors)

### Predictor Variables, Target Variable, and Distribution

```{r stats}
# country
free_gap %>%
  group_by(country) %>%
  summarize(n = n())
# year range
range(free_gap$year)
# CL
free_gap %>%
  group_by(CL) %>%
  summarize(n = n()) %>%
  ggplot(aes(x = CL, y = n)) +
  geom_bar(stat = "identity") +
  theme_stata() + # stata theme
  scale_colour_stata() +
  xlab("Degrees of Civil Liberties") +
  ylab("Count")
# PR
free_gap %>%
  group_by(PR) %>%
  summarize(n = n()) %>%
  ggplot(aes(x = PR, y = n)) +
  geom_bar(stat = "identity") +
  xlab("Degrees of Political Rights") +
  ylab("Count")
# Status
free_gap %>%
  group_by(Status) %>%
  summarize(n = n()) %>%
  ggplot(aes(x = Status, y = n)) +
  geom_bar(stat = "identity") +
  xlab("Status") +
  ylab("Count")
# is_ldc
free_gap %>%
  group_by(is_ldc) %>%
  summarize(n = n()) %>%
  ggplot(aes(x = is_ldc, y = n)) +
  geom_bar(stat = "identity") +
  xlab("Developed Country or Not") +
  ylab("Count")
# infant_mortality
free_gap %>%
  ggplot(aes(x = infant_mortality)) +
  geom_histogram(binwidth = 1, color = "black") +
  xlab("Infant Mortality")
# life_expectancy
free_gap %>% 
  ggplot(aes(x = life_expectancy)) +
  geom_histogram(binwidth = 1, color = "black") +
  xlab("Life Expectancy")
# fertility
free_gap %>%
  ggplot(aes(x = fertility)) +
  geom_histogram(binwidth = 1, color = "black") +
  xlab("Fertility")
# population
free_gap %>%
  ggplot(aes(x = log2(population))) +
  geom_histogram(binwidth = 1, color = "black") +
  xlab("Population (log transformed)")
# gdp
free_gap %>%
  ggplot(aes(x = log10(gdp))) +
  geom_histogram(color = "black") +
  xlab("GDP (log transformed)")
```

### Fertility, mortality, and life expectancy (Life Variables) by continent in 1995 and 2010

Compare each pair of Life variables to visually inspect the relationship between each variables.

```{r}
# fertility & infant_mortality
free_gap %>%
    filter(year %in% c(1995, 2010)) %>%
  ggplot(aes(fertility, infant_mortality, color = continent)) +
  geom_point() +
  theme_stata(scheme = "s1color") +
  ylab("infant mortality") +
  scale_color_stata("s1color") +
  facet_grid(.~year) 
```

```{r}
# life_expectancy $ infant_mortality
free_gap %>%
    filter(year %in% c(1995, 2010)) %>%
    ggplot(aes(life_expectancy, infant_mortality, color = continent)) + 
    geom_point() +
    facet_grid(.~year) +
    ylab("infant mortality") +
    theme_stata(scheme = "s1color") +
    scale_color_stata("s1color") 

```

```{r}
# life_expectancy & fertlity
free_gap %>% 
    filter(year %in% c(1995, 2010)) %>% 
    ggplot(aes(life_expectancy, fertility, color = continent)) + 
    geom_point() + 
    facet_grid(.~year) +
    xlab("life expectancy") +
    theme_stata(scheme = "s1color") +
    scale_color_stata("s1color") 
```

## Population & 3 Life Variables

```{r}
# log10(population) & infant_mortality - comparison of 1995 & 2010
free_gap %>%
    filter(year %in% c(1995, 2010)) %>%
    ggplot(aes(log10(population), infant_mortality, color = continent)) +
    geom_point() +
    facet_grid(.~year) +
    xlab("Population (log10)") +
    ylab("infant mortality") +
    theme_stata(scheme = "s1color") +
    scale_color_stata("s1color") 
```

```{r}
# log10(population) & life_expectancy
free_gap %>%
    filter(year %in% c(1995, 2010)) %>%
    ggplot(aes(log10(population), life_expectancy, color = continent)) +
    geom_point() +
    facet_grid(.~year) +
    xlab("Population (log10)") +
    ylab("life expectancy") +
    theme_stata(scheme = "s1color") +
    scale_color_stata("s1color") 
```

```{r}
# log10(population) & fertility
free_gap %>%
    filter(year %in% c(1995, 2010)) %>%
    ggplot(aes(log10(population), fertility, color = continent)) +
    geom_point() +
    facet_grid(.~year) +
    xlab("Population (log10)") +
    theme_stata(scheme = "s1color") +
    scale_color_stata("s1color") 
```

### GDP per capita & Other Predictors

```{r}
# log10(gdp_per_cap) distribution by Region 
free_gap %>%
    filter(year %in% c(1995, 2011)) %>% 
    mutate(region = reorder(region, gdp_per_cap, median)) %>%
    ggplot(aes(x = log10(gdp_per_cap), y = region)) +
    facet_grid(year ~.) +
    geom_point(size = 1, alpha = 0.5) +
    ylab("Population (Log Transformed)") +
    xlab("GDP per capita (Log Transformed)")
```

```{r}
# gdp_per_cap & life_expectancy in comparison of 1995 and 2010
free_gap %>%
  filter(year %in% c(1995, 2010)) %>%
  ggplot(aes(log10(gdp_per_cap), life_expectancy, color = continent)) +
  geom_point() +
  facet_grid(. ~ year) +
  xlab("GDP per capita (log transformed)") +
  ylab("Life Expectancy") +
  theme_stata(scheme = "s1color") +
  scale_color_stata("s1color") 
```

```{r}
# gdp_per_cap & fertility
free_gap %>%
  filter(year %in% c(1995, 2010)) %>%
  ggplot(aes(log10(gdp_per_cap), fertility, color = continent)) +
  geom_point() +
  facet_grid(. ~ year) +
  xlab("GDP per capita (log transformed)") +
  theme_stata(scheme = "s1color") +
  scale_color_stata("s1color") 
```

```{r}
# gdp_per_cap & mortality
free_gap %>%
  filter(year %in% c(1995, 2010)) %>%
  ggplot(aes(log10(gdp_per_cap), infant_mortality, color = continent)) +
  geom_point() +
  facet_grid(. ~ year) +
  xlab("GDP per capita (log transformed)") +
  theme_stata(scheme = "s1color") +
  scale_color_stata("s1color")
```

```{r}
# population & gdp - log10 transformed
free_gap %>%
  filter(year %in% c(1995, 2010)) %>%
  ggplot(aes(x = log10(population), y = log10(gdp), color = continent)) +
  geom_point() +
  facet_grid(. ~ year) +
  xlab("Population (log transformed)") +
  ylab("GDP (log transformed)") +
  theme_stata(scheme = "s1color") +
  scale_color_stata("s1color")
```

## GDP per cap & Freedom Index

```{r}
# gdp_per_cap distribution
free_gap %>%
  filter(year == 2010) %>%
  ggplot(aes(log2(gdp_per_cap))) +
  geom_histogram(binwidth = 1, color = "black") +
  xlab("GDP per capita (Log Transformed)") +
  ylab("Count")
```

```{r}
# CL & PR combo count
free_gap %>%
  count(CL, PR, sort = TRUE) 
```

```{r}
# adding Status & is_ldc
free_gap %>%
  count(CL, PR, Status, is_ldc) %>%
  arrange(desc(n))
```

### `dol_per_day` with categorical variables

```{r}
ggplot(free_gap, aes(x = Status, y = dol_per_day)) + geom_point(aes(colour = factor(Status)), cex = 1.0, pch = 1.0, position = position_jitter(w = 0.1, h = 0)) +
  xlab("Status") +
  ylab("Dollar per day")

#(cex = 1.0, pch = 1.0, position = position_jitter(w = 0.1, h = 0))


```

### Create Freedom Index Matrix

Select `CL`, `PR`, `is_ldc` variables to create Freedom Index Matrix.

```{r}
# Creat Freedom Index metrix
fi_gap <- free_gap %>% 
  dplyr::select(CL, PR, is_ldc) # extract 3 variables
# View(fi_gap)
```

```{r}
# Create 3D Scatterplot with CL, PR, and is_ldc
with(free_gap, scatterplot3d(CL, PR, is_ldc, pch = 19, box = FALSE, highlight.3d = TRUE))
```

## Principal Component Analysis

### Sub-setting free_gap data for PCA

```{r}
# omit non-numeric variables + dol_per_day
free_matrix <- free_gap %>%
  dplyr::select(-c(country, year, Status, continent, region, dol_per_day))
View(free_matrix)
```

```{r}
pca_model <- prcomp(free_matrix,
                    scale = TRUE,
                    center = TRUE)
summary_pca <- summary(pca_model)
summary_pca
View(summary_pca$x)
```

### Plotting PCA

```{r}
# Scree Plot for PCA
plot(pca_model, type = "l", main = "Scree Plot for PCA")
```

```{r}
plot(summary_pca$importance[3,], type = "l")
```

```{r}
# Plotting PC1 & PC2
pca_df <- data.frame(free_gap, pca_model$x)
ggplot(pca_df, aes(x = PC1, y = PC2, color = Status)) + geom_point(size = 0.5) + ggtitle("Plotting Status Data against PC1 and PC2")
```

# Classification Machine Learning

Create classification machine learning model to predict Freedom Status. The target variable is `Status`.

### Splitting free_gap data into testing and training set

```{r}
# data partition 40-60
set.seed(2022, sample.kind = "Rounding")
test_index <- createDataPartition(free_gap$Status,times = 1, p = 0.4, list = FALSE) # nolint
training <- free_gap[-test_index,]
testing <- free_gap[test_index,]
# remove duplicates
testing <- testing %>% semi_join(training, by = "Status")
```

```{r}
# Omit variables that are not needed + STATUS
training_2 <- training %>%
  dplyr::select(-c(country, year, Status, continent, region, dol_per_day))
testing_2 <- testing %>%
  dplyr::select(-c(country, year, Status, continent, region, dol_per_day))
```

**Scatter plot of matrices (SPLOM)** with bivariate scatter plots below the diagonal, histograms on the diagonal, and the Pearson correlation above the diagonal

```{r}
# psych library pairs.panels()
pairs.panels(training_2,
  gap = 0,
  bg = c("red", "blue", "yellow")[training$Status],
  stars = TRUE,
  pch = 21)
```

```{r}
# training PCA with 9 variables
pr <- prcomp(training_2,
  center = TRUE,
  scale. = TRUE)
attributes(pr)
pr_summary <- summary(pr)
pr_summary$importance
x <- pr$x

# testing PCA
pr_t <- prcomp(testing_2,
  center = TRUE,
  scale. = TRUE)
```

## Freedom Index Gap - PCA

```{r}
# freedom index gap PCA
pr_2 <- prcomp(fi_gap,
  center = TRUE,
  scale. = TRUE)
pr_2_summary <- summary(pr_2)
pr_2_summary$importance
x_2 <- pr_2$x
```

SPLOM graph on freedom gap index data (3 variables)

```{r}
# freedom gap 3 variables
pairs.panels(fi_gap,
  gap = 0,
  bg = c("red", "blue", "yellow")[training$Status],
  pch = 21)
```

```{r}
# x after PCA rolation 
# check multicollinearity
pairs.panels(pr_2$x,
  gap = 0,
  bg = c("red", "blue", "yellow")[training$Status],
  pch = 21)
```

No multicollinearity was detected.

#### Multinomial Prediction with PCA (Status - 3 levels)

We will run Multinomial Prediction using PCA rotated data.

```{r}
# training prediction
trg_pred <- predict(pr, training)
# add Status
trg_pred <- data.frame(trg_pred, training$Status)
# testing prediction
tst_pred <- predict(pr_t, testing)
tst_pred <- data.frame(tst_pred, testing$Status)

```

```{r}
trg_pred$training.Status <- relevel(trg_pred$training.Status, ref="NF")
trg_model <- nnet::multinom(training.Status ~ ., data = trg_pred)
summary(trg_model)
                   
```

### Evaluation Metrics: Confusion Matrix and Classification Errors

```{r}
p_hat <- predict(trg_model, trg_pred)
p_tab <- table(p_hat, trg_pred$training.Status)
p_tab
```

There are 5 classification errors.

### Support Vector Machines

Support Vector Machines are supervised learning models that analyze data for both classification and regression analysis.

```{r}
# use svm function from e1072 library

svm_fit <- svm(Status ~ .,
                 data = training, 
                 kernel = "linear", #C-classification
                 cost = 10,
                 scale = TRUE)
print(svm_fit)

svm_pred <- predict(svm_fit, newdata = testing, type = "response")
confusionMatrix(svm_pred, testing$Status)



```

I observe that the over-fitting is going on for all the classification models. I will review the models again and will replace with other classification models in the next revision.

# Regression Modeling

Create regression model to predict dollar per day metrics using Freedom index from each countries. The target variable is `dol_per_day`.

## `dol_per_day` Prediction with Freedom Gap Index

```{r}
# create gdp_per_cap pred data frame
dpd_df <- free_gap %>%
  dplyr::select(-c(country, year, Status, continent, region, gdp_per_cap))
summary(dpd_df)
```

## Correlation and Variance Analysis

### Pearson Correlation Coefficient & P-value

Pearson correlation coefficient measures the linear dependence between 2 variables. In this section, I will take some numerical variables (predictors) to calculate correlations with `dol_per_day` values.

```{r}
# the correlation between fertility and dol_per_day
cor.test(dpd_df$fertility, dpd_df$dol_per_day, method = "pearson")
```

```{r}
# correlation matrix
free_val <- cor(free_matrix)
corrplot(free_val, 
         type = "upper", 
         order = "hclust", 
         tl.col = "black",
         tl.srt = 50)
```

The `fertility` and `life_expectancy` have strongly negative or positive correlations with other predictor variables.

### Analysis of Variance (ANOVA)

ANOVA is a statistical test for estimating how a quantitative dependent variable changes according to the levels of one or more **categorical independent variables**. ANOVA tests whether there is a difference in means of the groups at each level of the independent variable. The null hypothesis (H~0~) means no difference in means. The Alternative hypothesis (H~a~) means that the means are different from one another.

In this section, I will pick several categorical variables to evaluate how these variables and their categorical levels have influence on the dependent (target) variable.

```{r ANOVA library}
library(ggplot2)
library(ggpubr)
library(tidyverse)
library(broom)
library(AICcmodavg)
```

#### One-Way Anova

In one-way Anova, we are modeling `dol_per_day` as a function of the type of `Status` variable.

```{r}
anova <- aov(formula = dol_per_day ~ Status, data = free_gap)
summary(anova)
```

In the summary of Anova, we will look at F-test score and P-value for the F-test score.

A larger F-score means there is a larger difference between the means, and more likely it is that the variation caused by the independent variable is true and not due to chance. P-value shows how likely it is that the F-test score calculated from the test would have occurred if the null hypothesis of no difference among group means were true.

For the One-way Anova above, the P-value of `Status` variable is incredibly small, so it appears that the type of `Status` has a significant impact on the `dol_per_day`.

```{r}
anova_pr <- aov(formula = dol_per_day ~ PR, data = free_gap)
summary(anova_pr)
```

This time we run `PR` instead of `Status`. In next section, we will add `PR` and `CL` for Two-Way Anova.

#### Two-Way Anova

In Two-Way Anova, we will model `dol_per_day` as a function of type of `CL` and `PR` variables.

```{r}
anova2 <- aov(formula = dol_per_day ~ CL + PR, data = free_gap)
summary(anova2)
```

Adding `CL` variable seems to make the model better than the One-Way model. The Residual Sum of Squares went from 1533330 to 1274044, and both `CL` and `PR` are statistically significant.

#### Akaike Information Criterion (AIC)

In this section, we are comparing all 3 Anova models above to test which model fits the best.

```{r}
mod_set <- list(anova, anova_pr, anova2)
mod_name <- c("one way", "one way-PR","two way")
aictab(mod_set, modnames = mod_name)
```

We are looking at the values of AIC from the output, and choose the one with the lowest AIC value. A lower AIC value means more information explained.

According to the result above, it appears that the Two-Way model is the best fit.

#### Run a Diagnostic Plot

```{r}
# check for homoscedasticity
par(mfrow=c(2,2))
plot(anova2)
par(mfrow=c(1,1))

```

The diagnostic plots show the unexplained variance/residuals across the range of data. The red line represent the mean of the residuals, and it should be horizontal and centered on zero, which means that there are no large outliers that would cause bias in the model.

The Q-Q plot should have a line closer to a slop of 1. However the plot above shows that the slope is far from 1. In that case, the model appears to not fit the assumption of homoscedasticity.

### Splitting the data set

```{r}
# split trg and tst data frame
set.seed(2022, sample.kind = "Rounding")
test_index <- createDataPartition(dpd_df$dol_per_day, 
                                  times = 1, 
                                  p = 0.2, 
                                  list = FALSE)
trg <- dpd_df[-test_index,]
tst <- dpd_df[test_index,]
```

### Scaling Data Sets

```{r}
trg[-9] = scale(trg[-9])
tst[-9] = scale(tst[-9])
```

### Evaluation Matrix: RMSE, R-Squared, and MSE

**Root Mean Square Error (RMSE)** is the standard deviation of the residuals (errors). RMSE measures how spread out these residuals are, and it tells you how concentrated the data is around the predicted line of best fit.

**Mean Squared Error (MSE)** measures the average of the squared of the residuals (errors). MSE is a risk function, corresponding to the expected value of the squared error loss.

**R-squared (R\^2)** is a goodness of fit. It measures the proportion of the variance for a dependent variable that's explained by an independent variables in a regression model.

```{r}
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}

R_sq <- function(true_ratings, predicted_ratings){
  cor(true_ratings, predicted_ratings)^2
}

MSE <- function(true_ratings, predicted_ratings){
  mean(true_ratings - predicted_ratings)^2
}

```

### Linear Baseline Model

```{r}
train_lm <- lm(dol_per_day~., data = trg)
plot(train_lm)

pred_lm <- predict(train_lm, newdata = tst) 
# pred_lm <- predict(train_lm, newdata = tst, interval = 'confidence')

```

Based on the Q-Q plot and residual plots above, the relationship between predictor variables and target variable is highly likely nonlinear or complex, and might be difficult to model parametrically. The residuals of a linear regression violate assumption of normality.

These observations are the good indicators to use some of the complex training models such as Generalized Additive Model and Decision Trees.

```{r}
# calculate R-squared
R_sq(tst$dol_per_day, pred_lm)
# calculate MSE
MSE(tst$dol_per_day, pred_lm)

```

### Generalized Additive Models (GAM)

GAM are useful when we suspect that the relationship between the predictor and target variables may not be linear but it might be a smooth curved relationship.

```{r}
# 
train_gam <- gam(dol_per_day ~ CL + PR + is_ldc + infant_mortality + life_expectancy + fertility, 
                 family = gaussian, 
                 data = trg)

# Summary of train_gam
summary(train_gam)

# output prediction
pred_gam <- predict(train_gam, newdata = tst)

# calculate R-squared
R_sq(tst$dol_per_day, pred_gam)
# calculate MSE
MSE(tst$dol_per_day, pred_gam)

```

### Decision Trees

Decision Trees model is also useful when we try to capture a suspected nonlinear relationship between complex predictor and target variables. The model can also handle a larger dataset efficiently which makes it a better choice for big data application.

```{r}
# decision tree libraries
library(rpart)
library(rpart.plot)
# fit rpart model
train_cart <- rpart(dol_per_day ~ ., data = trg)
```

```{r}
# create a decision tree with training data
rpart.plot(train_cart,box.palette = 'RdBu',shadow.col = "grey",nn = TRUE)
```

```{r}
# Predicted target values
pred_cart <- predict(train_cart, newdata = tst)

```

```{r}
# Calculate R-Squared
R_sq(tst$dol_per_day, pred_cart)
# Calculate MSE
MSE(tst$dol_per_day, pred_cart)
```

### Support Vector Machine for Regression

Support Vector Machine model is also useful to capture nonlinear relationships between complex predictor and target variables, without assuming a specific functional form. It is able to perform the training by mapping the data into a high-dimensional feature space.

**Parameter Tuning**

```{r}
# tuning SVM
tuneResult <- tune(svm, dol_per_day ~ ., data = trg, trace = FALSE, ranges = list(epsilon = seq(0, 0.3, 0.01)))
print(tuneResult)
plot(tuneResult)

# best parameter: 0.13
# best performance: 182
```

The best parameter is 0.13, and the best performance is 182.

```{r}
# fitting Linear with nu-regression
train_svm_reg <- svm(dol_per_day ~ ., data = trg, kernel = "linear", type = "nu-regression", epsilon = 0.13)
pred_svm_reg <- predict(train_svm_reg, newdata = tst)
```

```{r}
summary(train_svm_reg)
```

```{r}
# Evaluating Linear
R_sq(tst$dol_per_day, pred_svm_reg)
# calculate MSE
MSE(tst$dol_per_day, pred_svm_reg)
```

Fitting Polynomial

```{r}
# Polynomial

train_svm_reg <- svm(dol_per_day ~ ., data = trg, kernel = "polynomial",type = "eps-regression", degree = 3, epsilon = 0.13)
pred_svm_reg <- predict(train_svm_reg, newdata = tst)
```

```{r}
print(train_svm_reg)
```

```{r}
# Evaluating Polynomial
R_sq(tst$dol_per_day, pred_svm_reg)
# calculate MSE
MSE(tst$dol_per_day, pred_svm_reg)

```

## Summary

Based on the evaluation of R-Squared and MSE for each training algorithms, Decision Trees model has the best performing training model with R-Squared, 0.7741318, and MSE, .01941686.

## References

[ANOVA in R \| A Complete Step-by-Step Guide with Examples (scribbr.com)](https://www.scribbr.com/statistics/anova-in-r/)

[Regression -- unfold your data (snowflect.com)](http://snowflect.com/regression/#:~:text=In%20%E2%80%98eps-regression%E2%80%99%2C%20selection%20of%20the%20number%20of%20support,application%2C%20default%20values%20are%20used%20for%20both%20parameters.)
